# chinese-llama2.c
æ”¯æŒä¸­æ–‡çš„ llama2.c


## ç®€ä»‹

[llama2.c](https://github.com/karpathy/llama2.c) é¡¹ç›®æä¾›äº†å¤§è¯­è¨€æ¨¡å‹ä»è®­ç»ƒåˆ°éƒ¨ç½²çš„å®Œæ•´çš„æŠ€æœ¯æ ˆï¼Œä½¿å¾—ä¸ªäººå¯ä»¥ä½“éªŒå¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†ã€‚

ä½œä¸ºä¸€ä¸ªå­¦ä¹ é¡¹ç›®ï¼Œæœ¬é¡¹ç›®æŠŠåœºæ™¯æ‹“å±•åˆ°ä¸­æ–‡ï¼Œè®­ç»ƒä¸­æ–‡ç‰ˆçš„ tinyllamasã€‚

è®­ç»ƒæ•°æ®æ˜¯å°†è‹±æ–‡ç‰ˆçš„ [TinyStories](https://huggingface.co/datasets/roneneldan/TinyStories) (500ä¸‡æ ·æœ¬) ç¿»è¯‘æˆä¸­æ–‡ [tinyllamas-chinese](https://huggingface.co/adam89/tinyllamas-chinese) (ç¿»è¯‘äº†çº¦400ä¸‡)ï¼Œè·å¾—çº¦ 1G å­—ç¬¦çš„è®­ç»ƒæ•°æ®ã€‚åœ¨æ­¤åŸºç¡€ä¸Šè®­ç»ƒå¾—åˆ°èƒ½å¤Ÿæµç•…ç”Ÿæˆä¸­æ–‡å°æ•…äº‹çš„è¯­è¨€æ¨¡å‹ï¼Œæ¬¢è¿å¤§å®¶è¯•ç©ï¼


## è¯•ç©
```bash
wget https://huggingface.co/adam89/tinyllamas-chinese/resolve/main/stories15M.bin
make run
./run stories15M.bin -z tok5000.bin
```

```text
ä»å‰ï¼Œæœ‰ä¸€ä¸ªå°å¥³å­©å«è‰è‰ã€‚å¥¹å–œæ¬¢åœ¨å¤–é¢ç©ï¼Œæ¢ç´¢å¥¹å®¶åé¢çš„æ ‘æ—ã€‚æœ‰ä¸€å¤©ï¼Œå¥¹åœ¨åœ°ä¸Šå‘ç°äº†ä¸€ä¸ªé—ªé—ªå‘å…‰çš„é’‰å­ã€‚å¥¹æ¡èµ·æ¥ç»™å¦ˆå¦ˆçœ‹ã€‚
"çœ‹ï¼Œå¦ˆå¦ˆï¼æˆ‘æ‰¾åˆ°ä¸€ä¸ªé’‰å­ï¼" è‰è‰å…´å¥‹åœ°å–Šé“ã€‚
"é‚£æ˜¯ä¸€ä¸ªæ¼‚äº®çš„é’‰å­ï¼Œè‰è‰ã€‚å°å¿ƒåˆ«æ‰ä¸‹æ¥ã€‚" å¦ˆå¦ˆè¯´ã€‚
è‰è‰çœ‹äº†çœ‹å®ƒï¼Œç„¶åè¯´ï¼š"æˆ‘æƒ³æŠŠå®ƒæ”¾åœ¨æˆ‘çš„å°ç›’å­é‡Œã€‚æˆ‘æƒ³æŠŠå®ƒä¿ç®¡å¥½ï¼Œè®©æˆ‘çš„æœ‹å‹ä»¬çŸ¥é“å®ƒå¾ˆé‡è¦ã€‚"
"é‚£æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ä¸»æ„ï¼Œè‰è‰ã€‚è°¢è°¢ä½ è¿™ä¹ˆå¥½çš„å¸®æ‰‹ã€‚" å¦ˆå¦ˆè¯´ã€‚
è‰è‰ç¬‘äº†ç¬‘ï¼Œç„¶åç»§ç»­æ‹¿ç€é’‰å­å’Œç›’å­ç©ï¼Œå‡†å¤‡å¥½å¥½åœ°ç¡ä¸€æ™šã€‚
```

ä¹Ÿå¯å¢åŠ  promptï¼Œè®©æ¨¡å‹åšæ•…äº‹ç»­å†™ï¼š

```
./run stories15M.bin -z tok5000.bin -i 'ä»å‰ï¼Œæœ‰ä¸€åªä¼šé£çš„ç‹—å«æ³¢æ³¢'
```

```text
ä»å‰ï¼Œæœ‰ä¸€åªä¼šé£çš„ç‹—å«æ³¢æ³¢ã€‚æ³¢æ³¢å–œæ¬¢è§‚å¯Ÿå¤©ç©ºä¸­çš„æ˜Ÿæ˜Ÿã€‚ä»–åœ¨æ˜Ÿæ˜Ÿä¸‹ä»°æœ›æ˜Ÿç©ºï¼Œçœ‹åˆ°å®ƒä»¬é—ªçƒå¹¶å‘å‡ºå“äº®çš„å™ªéŸ³ã€‚

æœ‰ä¸€å¤©ï¼Œæ³¢æ³¢çœ‹åˆ°äº†ä¸€ä¸ªåå«è‹çš„å°å¥³å­©ã€‚è‹æ­£åœ¨ç”»ä¸€æ£µå¤§æ ‘ã€‚æ³¢æ³¢ä¹Ÿæƒ³ç”»è¿™æ£µæ ‘ã€‚è‹ç¬‘äº†ç¬‘ï¼Œè¯´ï¼šâ€œæ³¢æ³¢ï¼Œæˆ‘å¯ä»¥å’Œä½ ä¸€èµ·ç”»å—ï¼Ÿâ€æ³¢æ³¢æ‘‡äº†æ‘‡å°¾å·´ï¼Œé£åˆ°äº†è‹çš„æ‰‹ä¸Šã€‚

è‹å’Œæ³¢æ³¢ä¸€èµ·ç”»æ ‘ã€‚ä»–ä»¬ç”¨å½©è‰²çš„é¢œæ–™ç»™æ ‘æ¶‚ä¸Šäº†è‰²å½©ã€‚ä»–ä»¬å¯¹è‡ªå·±çš„ç”»éå¸¸æ»¡æ„ã€‚è‹çœ‹ç€ç”»è¯´ï¼šâ€œå“‡ï¼Œæ³¢æ³¢ï¼ä½ çœŸæ£’ï¼â€æ³¢æ³¢ä¸ºä»–çš„æœ‹å‹æ„Ÿåˆ°éª„å‚²ï¼Œä»–ä»¬ä¸€èµ·æ¬£èµäº†è¿™æ£µæ ‘ã€‚
```

å¦‚æœæƒ³è·å¾—æ›´å¥½çš„æ•ˆæœï¼Œå¯ä»¥å°è¯•æ›´å¤§çš„ 110M æ¨¡å‹ï¼š
```
./run stories110M.bin -z tok5000.bin -t 0 -i 'ä»å‰ï¼Œæœ‰ä¸€åªä¼šé£çš„ç‹—å«æ³¢æ³¢'
```

```
ä»å‰ï¼Œæœ‰ä¸€åªä¼šé£çš„ç‹—å«æ³¢æ³¢ã€‚æ³¢æ³¢å–œæ¬¢åœ¨å¤©ç©ºä¸­é£ç¿”ã€‚æœ‰ä¸€å¤©ï¼Œä»–çœ‹åˆ°äº†ä¸€åªåå«å¥‡æ™®çš„å°é¸Ÿã€‚å¥‡æ™®å¾ˆä¼¤å¿ƒï¼Œå› ä¸ºå¥¹ä¸èƒ½åƒæ³¢æ³¢ä¸€æ ·é£å¾—é«˜ã€‚

æ³¢æ³¢è¯´ï¼šâ€œå¥‡æ™®ï¼Œæˆ‘å¯ä»¥å¸®åŠ©ä½ é£å¾—é«˜é«˜çš„ã€‚æˆ‘ä»¬å¯ä»¥ä¸€èµ·é£ï¼â€å¥‡æ™®å¾ˆé«˜å…´åœ°è¯´ï¼šâ€œå¥½çš„ï¼Œè¯·å¸®å¸®æˆ‘ï¼Œæ³¢æ³¢ï¼â€ ä»–ä»¬å¼€å§‹ä¸€èµ·é£è¡Œï¼Œå¥‡æ™®éå¸¸åŠªåŠ›åœ°å°è¯•ç€é£å¾—æ›´é«˜ã€‚

ä½†æ˜¯å¥‡æ™®ä¸å¤Ÿå¼ºå£®ï¼Œå¥¹æ— æ³•åƒæ³¢æ³¢ä¸€æ ·é£å¾—é«˜ã€‚å¥‡æ™®æ‘”å€’äº†ï¼Œå—ä¼¤äº†ã€‚æ³¢æ³¢ä¸ºä»–çš„æ–°æœ‹å‹æ„Ÿåˆ°éš¾è¿‡ã€‚ä»–ä»¬éƒ½æ˜ç™½ï¼Œæœ‰æ—¶å€™ï¼Œå³ä½¿ä½ å°½åŠ›å»åšï¼Œäº‹æƒ…ä¹Ÿä¸æ€»æ˜¯å¦‚ä½ æ‰€æ„¿ã€‚
``` 
è¿˜æ˜¯è›®æœ‰å“²ç†çš„ğŸ˜‚

## æ¨¡å‹
ä»¿ç…§ [llama2.c](https://github.com/karpathy/llama2.c) åŸé¡¹ç›®ï¼Œè¿™é‡Œä¹Ÿè®­ç»ƒäº†ä¸‰ä¸ªè§„æ¨¡çš„ä¸­æ–‡æ¨¡å‹ï¼Œæ‰˜ç®¡åœ¨huggingface hub [tinyllamas-chinese](https://huggingface.co/adam89/tinyllamas-chinese)ã€‚
| model | dim | n_layers | n_heads | n_kv_heads | max context length | parameters | val loss | download
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 15M | 288 | 6 | 6 | 6 | 256 | 15M | 1.0868 | [stories15M.bin](https://huggingface.co/adam89/tinyllamas-chinese/resolve/main/stories15M.bin) |
| 42M| 512 | 8 | 8 | 8 | 1024 | 42M | 0.8907 | [stories42M.bin](https://huggingface.co/adam89/tinyllamas-chinese/resolve/main/stories42M.bin) |
| 110M| 768 | 12 | 12 | 12 | 1024 | 110M | 0.8205 | [stories110M.bin](https://huggingface.co/adam89/tinyllamas-chinese/resolve/main/stories110M.bin) |

## è‡ªå·±ä»å¤´è®­ç»ƒ

### è®­ç»ƒ tokenizer
```bash
python3 tinystories.py download
python3 tinystories.py train_vocab --vocab_size=5000
python3 tinystories.py pretokenize --vocab_size=5000
```

å°† tokenizer å¯¼å‡ºä¸º`.bin`æ ¼å¼ï¼š
```
python3 tokenizer.py --tokenizer-model=data/tok5000.model
```

### è®­ç»ƒæ¨¡å‹

```bash
# 15Mæ¨¡å‹
torchrun --standalone --nproc_per_node=4 train.py --vocab_source=custom --vocab_size=5000 --batch_size=128 --n_layers=6 --n_heads=6 --n_kv_heads=6 --max_seq_len=256 --gradient_accumulation_steps=4 --max_iters=100000

# 42Mæ¨¡å‹
torchrun --standalone --nproc_per_node=4 train.py --vocab_source=custom --vocab_size=5000 --batch_size=128 --dim=512 --n_layers=8 --n_heads=8 --n_kv_heads=8 --max_seq_len=1024 --gradient_accumulation_steps=4 --max_iters=20000

# 110Mæ¨¡å‹
torchrun --standalone --nproc_per_node=4 train.py --vocab_source=custom --vocab_size=5000 --batch_size=128 --dim=768 --n_layers=12 --n_heads=12 --n_kv_heads=12 --max_seq_len=1024 --gradient_accumulation_steps=4 --max_iters=20000
```

è®­ç»ƒè¯¦æƒ…
| æ¨¡å‹ | æ˜¾å­˜å ç”¨|  epochs | tokens_per_iter | max_iters | num_tokens | è®­ç»ƒæ—¶é•¿ |
| - | - | - | - | - | - | - |
| 15M | 6.5G | 13 | 131,072 | 100,000 | 1G | ~1h |
| 42M | 30.5G |  10.5 | 524,288 | 20,000 | 1G | ~1.5h |
| 110M | 53.5G | 10.5 | 524,288 | 20,000 | 1G | ~3.4h |